{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvuEP0kLpV8T"
   },
   "source": [
    "**Submitted By: PARVEJ ANSARI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lko5IojpYrw"
   },
   "source": [
    "Q1. Explain the architecture of BERT\n",
    "\n",
    "Ans: \n",
    "\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. BERT is based on the Transformer architecture\n",
    "\n",
    "![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfvPugnWpk7-"
   },
   "source": [
    "Q2.  Explain Masked Language Modeling (MLM)\n",
    "\n",
    "Ans:\n",
    "\n",
    "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary.\n",
    "\n",
    "![0_V0GyOt3LoDVfY7y5.png](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPdzOnT8pk5f"
   },
   "source": [
    "Q3.  Explain Next Sentence Prediction (NSP)\n",
    "\n",
    "Ans:\n",
    "\n",
    "Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling — MLM). Where MLM teaches BERT to understand relationships between words — NSP teaches BERT to understand longer-term dependencies across sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVP_o59x6Cmi"
   },
   "source": [
    "Q4. What is Matthews evaluation?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table. A correlation of: C = 1 indicates perfect agreement, C = 0 is expected for a prediction no better than random, and C = -1 indicates total disagreement between prediction and observation\n",
    "\n",
    "![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/875/1*8E2rPn_ccOqGuPYj1gBTAg.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V1aotumpk3C"
   },
   "source": [
    "Q5. What is Matthews Correlation Coefficient (MCC)?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table. A correlation of: C = 1 indicates perfect agreement, C = 0 is expected for a prediction no better than random, and C = -1 indicates total disagreement between prediction and observation\n",
    "\n",
    "![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/875/1*8E2rPn_ccOqGuPYj1gBTAg.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3bo17oM6RGp"
   },
   "source": [
    "Q6. Explain Semantic Role Labeling\n",
    "Ans:\n",
    "\n",
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-ylSkrghF_O"
   },
   "source": [
    "Q7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "\n",
    "Ans:\n",
    "\n",
    "Specifically, we will take the pre-trained BERT model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? When the model is trained on a large generic corpus, it is called 'pre-training'. When it is adapted to a particular task or dataset it is called as 'fine-tuning'. Hence Fine-tuning a BERT model takes less time than pretraining. Below are other reasons:\n",
    "\n",
    "#### Easy Training\n",
    "\n",
    "First, the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!). \n",
    "\n",
    "#### Less Data\n",
    "\n",
    "In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data.\n",
    "\n",
    "#### Good Results\n",
    "\n",
    "Second, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative.\n",
    "\n",
    "##Recognizing Textual Entailment (RTE)\n",
    "\n",
    "Textual entailment recognition is the task of deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text (see the Instructions tab for the specific operational definition of textual entailment assumed in the challenge).\n",
    "\n",
    "##Explain the decoder stack of  GPT models.\n",
    "\n",
    "GPT model was based on Transformer architecture. It was made of decoders stacked on top of each other (12 decoders). ... GPT model works on a principle called autoregressive which is similar to one used in RNN. It is a technique where the previous output becomes current input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "Recognizing Textual Entailment (RTE) is a natural language processing (NLP) task that involves determining whether a given piece of text, known as the hypothesis, can be inferred or logically derived from another piece of text, known as the premise.\n",
    "\n",
    "For example, consider the premise \"The cat is on the mat\" and the hypothesis \"The mat has a cat on it.\" If we can logically infer the hypothesis from the premise, then we say that the entailment holds, and the task is considered successful.\n",
    "\n",
    "RTE is a challenging problem for NLP systems because it requires not only an understanding of the individual words and their meanings but also the relationships between them and the broader context in which they appear. RTE has numerous practical applications, including question answering, summarization, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Explain the decoder stack of GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "The decoder stack in GPT (Generative Pre-trained Transformer) models refers to the set of layers that process the input sequence of tokens and generate the output sequence of tokens. In GPT models, the decoder stack is composed of multiple layers of self-attention and feedforward neural networks, and is similar to the decoder stack used in the original Transformer architecture.\n",
    "\n",
    "The decoder stack in GPT models is typically made up of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a feedforward neural network. During training, the decoder stack takes the input sequence of tokens and generates a sequence of hidden representations that capture the contextual information of the input. This sequence of hidden representations is then fed into the output layer, which produces a probability distribution over the vocabulary for each position in the output sequence. During inference, the decoder stack generates the output sequence one token at a time, with each token being conditioned on the previously generated tokens.\n",
    "\n",
    "One notable feature of the decoder stack in GPT models is its autoregressive nature, which means that it generates the output sequence one token at a time, conditioned on the previously generated tokens. This makes it well-suited for tasks such as language modeling and text generation, where the goal is to generate coherent and fluent text.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
