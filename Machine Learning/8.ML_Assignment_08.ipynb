{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {
    "id": "b2aa57f2"
   },
   "source": [
    "# Assignment 08 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9jqN0jDNi69",
   "metadata": {
    "id": "w9jqN0jDNi69"
   },
   "source": [
    "SUBMITTED BY: PARVEJ ANSARI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4f3bd",
   "metadata": {
    "id": "19d4f3bd"
   },
   "source": [
    "##### 1. What exactly is a feature? Give an example to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1cdb",
   "metadata": {
    "id": "797d1cdb"
   },
   "source": [
    "**Ans:** Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning. \n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project.\n",
    "\n",
    "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bc9d9",
   "metadata": {
    "id": "941bc9d9"
   },
   "source": [
    "##### 2. What are the various circumstances in which feature construction is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0d0e7",
   "metadata": {
    "id": "08f0d0e7"
   },
   "source": [
    "**Ans:** The features in your data will directly influence the predictive models you use and the results you can achieve. Our results are dependent on many inter-dependent properties. We need great features that describe the structures inherent in your data. Better features means flexibility. The process of generating new variables (features) based on already existing variables is known as feature construction.\n",
    "\n",
    "Feature Construction is a useful process as it can add more information and give more insights of the data we are dealing with. It is done by transforming the numerical features into categorical features which is done while performing Binning. Also, feature construction is done by decomposing variables so that these new variables can be used in various machine learning algorithms such as the creation of Dummy Variables by performing Encoding. Other ways of constructing include deriving features from the pre-existing features and coming up with more meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a7ce",
   "metadata": {
    "id": "3a53a7ce"
   },
   "source": [
    "##### 3. Describe how nominal variables are encoded ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1d24",
   "metadata": {
    "id": "0b3b1d24"
   },
   "source": [
    "**Ans:** Nominal data is made of discrete values with no numerical relationship between the different categories —  mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish. Ordinal or Label Encoding can be used to transform non-numerical labels into numerical labels (or nominal categorical variables). Numerical labels are always between 1 and the number of classes. The labels chosen for the categories have no relationship. So categories that have some ties or are close to each other lose such information after encoding. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4555",
   "metadata": {
    "id": "c4de4555"
   },
   "source": [
    "##### 4. Describe how numeric features are converted to categorical features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ce760",
   "metadata": {
    "id": "7a2ce760"
   },
   "source": [
    "**Ans:** Numeric Features can be converted to Categorical Features using Binning. Discretization: It is the process of transforming continuous variables into categorical variables by creating a set of intervals, which are contiguous, that span over the range of the variable’s values. It is also known as “Binning”, where the bin is an analogous name for an interval.\n",
    "\n",
    "Benefits of this method are:\n",
    "1. Handles the Outliers in a better way.\n",
    "2. Improves the value spread.\n",
    "3. Minimize the effects of small observation errors.\n",
    "\n",
    " ables.\n",
    "\n",
    " \n",
    "Techniques to Encode Numerical Columns:\n",
    "\n",
    "(a) Equal width binning: It is also known as “Uniform Binning” since the width of all the intervals is the same. The algorithm divides the data into N intervals of equal size. The width of intervals is:\n",
    "\n",
    "   w=(max-min)/N\n",
    "\n",
    "Therefore, the interval boundaries are:[min+w], [min+2w], [min+3w],..., [min+(N-1)w] where, min and max are the minimum and maximum value from the data respectively. This technique does not changes the spread of the data but does handle the outliers.\n",
    "\n",
    "(b) Equal frequency binning: It is also known as “Quantile Binning”. The algorithm divides the data into N groups where each group contains approximately the same number of values.\n",
    "\n",
    "Consider, we want 10 bins, that is each interval contains 10% of the total observations. Here the width of the interval need not necessarily be equal.\n",
    "Handles outliers better than the previous method and makes the value spread approximately uniform(each interval contains almost the same number of values).\n",
    "\n",
    "(c) K-means binning: This technique uses the clustering algorithm namely ” K-Means Algorithm”. This technique is mostly used when our data is in the form of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109820",
   "metadata": {
    "id": "d6109820"
   },
   "source": [
    "##### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de65b0",
   "metadata": {
    "id": "72de65b0"
   },
   "source": [
    "**Ans:** Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy.\n",
    "\n",
    "**Most commonly used techniques under wrapper methods are:**\n",
    "\n",
    "1.**Forward selection**: In forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.\n",
    "\n",
    "2.**Backward elimination**: In backward elimination, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest p-value(> significance level). This process repeats again and again until we have the final set of significant features\n",
    "\n",
    "3.**Bi-directional elimination(Stepwise Selection):** It is similar to forward selection but the difference is while adding a new feature it also checks the significance of already added features and if it finds any of the already selected features insignificant then it simply removes that particular feature through backward elimination. Hence, It is a combination of forward selection and backward elimination.\n",
    "\n",
    "\n",
    "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://editor.analyticsvidhya.com/uploads/46072IMAGE2.gif)\\\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51584401",
   "metadata": {
    "id": "51584401"
   },
   "source": [
    "##### 6. When is a feature considered irrelevant? What can be said to quantify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a92af6",
   "metadata": {
    "id": "a2a92af6"
   },
   "source": [
    "**Ans:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise. \n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree).\n",
    "\n",
    "p-value greater than 0.05 means that the feature is insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21fc3f",
   "metadata": {
    "id": "3d21fc3f"
   },
   "source": [
    "##### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce42e",
   "metadata": {
    "id": "8d0ce42e"
   },
   "source": [
    "**Ans:** If two features `{X1, X2}` are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features. \n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes\n",
    "\n",
    "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://slideplayer.com/slide/4394644/14/images/3/Background+Relevance+between+features+Correlation+F-statistic.jpg)\\\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a123",
   "metadata": {
    "id": "7bd4a123"
   },
   "source": [
    "##### 8. What are the various distance measurements used to determine feature similarity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d5c4",
   "metadata": {
    "id": "ed81d5c4"
   },
   "source": [
    "**Ans:** Four of the most commonly used distance measures in machine learning are as follows: \n",
    "- Hamming Distance: Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n",
    "\n",
    "- Euclidean Distance: Calculates the distance between two real-valued vectors.\n",
    "- Manhattan Distance: Also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n",
    "\n",
    "- Minkowski Distance: Minkowski distance calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6004e",
   "metadata": {
    "id": "e4d6004e"
   },
   "source": [
    "##### 9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39944ba",
   "metadata": {
    "id": "a39944ba"
   },
   "source": [
    "**Ans:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Euclidean distance is the shortest path between source and destination which is a straight line as shown in Figure 1.3. but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines\n",
    "\n",
    "\"\\\"![0_V0GyOt3LoDVfY7y5.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQfHj0UiFFvu4iz6l4sjinKw11BtBgjpAjiTw&usqp=CAU)\\\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5cfd2",
   "metadata": {
    "id": "d8a5cfd2"
   },
   "source": [
    "##### 10. Distinguish between feature transformation and feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfcff",
   "metadata": {
    "id": "722dfcff"
   },
   "source": [
    "**Ans:** Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
    "\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "Feature Transformation is a technique by which we can boost our model performance. Feature transformation is a mathematical transformation in which we apply a mathematical formula to a particular column(feature) and transform the values which are useful for our further analysis. It is also known as Feature Engineering, which is creating new features from existing features that may help in improving the model performance. It refers to the family of algorithms that create new features using the existing features. These new features may not have the same interpretation as the original features, but they may have more explanatory power in a different space rather than in the original space. This can also be used for Feature Reduction. It can be done in many ways, by linear combinations of original features or by using non-linear functions. It helps machine learning algorithms to converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75388f58",
   "metadata": {},
   "source": [
    "##### 11. Make brief notes on any two of the following:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8377b7c4",
   "metadata": {},
   "source": [
    "**1.SVD (Singular Value Decomposition)**\n",
    "\n",
    "Singular Value Decomposition (SVD) is a matrix factorization technique that decomposes a rectangular matrix into three matrices, representing the matrix's singular values, left singular vectors, and right singular vectors.\n",
    "\n",
    "Given a matrix A, its SVD can be expressed as: A = UΣV^T, where U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of A.\n",
    "\n",
    "The singular values are non-negative real numbers, and they represent the importance or weight of each singular vector in the matrix. The left singular vectors and right singular vectors are also orthonormal, and they represent the directions in which the matrix A has the most significant variation.\n",
    "\n",
    "SVD has many applications in various fields such as signal processing, image compression, data analysis, and machine learning. It can be used for data reduction, noise reduction, feature extraction, and recommender systems, among other applications. SVD is a fundamental tool in linear algebra and has various extensions, such as truncated SVD, randomized SVD, and sparse SVD.\n",
    "\n",
    "**2. Collection of features using a hybrid approach**\n",
    "\n",
    "A hybrid approach for feature selection or feature extraction involves combining multiple methods or algorithms to identify the most relevant and informative features for a given problem. The idea is to leverage the strengths of each approach to improve the overall performance of the feature collection process.\n",
    "\n",
    "For example, one possible hybrid approach is to first apply a filter method, such as correlation-based feature selection, to eliminate irrelevant or redundant features based on statistical measures. Then, a wrapper method, such as recursive feature elimination, can be used to evaluate the remaining features using a predictive model and select the ones that result in the best performance.\n",
    "\n",
    "Another possible hybrid approach is to combine unsupervised and supervised methods. In this case, unsupervised techniques, such as principal component analysis or clustering, can be used to reduce the dimensionality of the feature space and identify groups of related features. Then, supervised methods, such as decision trees or support vector machines, can be used to select the most discriminative features within each group.\n",
    "\n",
    "Hybrid feature selection methods can be more effective than single methods alone because they can overcome their limitations and biases. However, it is important to carefully select and validate the methods used in the hybrid approach to avoid overfitting and ensure that the resulting feature set is meaningful and interpretable.\n",
    "\n",
    "**3. The width of the silhouette**\n",
    "\n",
    "The width of the silhouette is a measure used in cluster analysis to assess the quality of a clustering solution. It is based on the silhouette coefficient, which measures how well each data point fits within its assigned cluster compared to other clusters.\n",
    "\n",
    "To compute the silhouette coefficient for a data point i, we first calculate two distances: a(i), which is the average distance from i to all other points in the same cluster, and b(i), which is the minimum average distance from i to all points in any other cluster. The silhouette coefficient for i is then defined as: (b(i) - a(i)) / max(a(i), b(i))\n",
    "\n",
    "The width of the silhouette is defined as the average silhouette coefficient across all data points in the dataset. A value of 1 indicates that the clustering is perfect, while a value of -1 indicates that the clustering is completely arbitrary or incorrect. A value close to 0 indicates that there is significant overlap or ambiguity in the clustering.\n",
    "\n",
    "The width of the silhouette can be used to compare different clustering algorithms or parameter settings and to determine the optimal number of clusters for a given dataset. However, it should be used in conjunction with other evaluation measures and domain knowledge to ensure that the resulting clusters are meaningful and useful.\n",
    "\n",
    "**4. Receiver operating characteristic curve**\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "A Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier system that shows the trade-off between its sensitivity and specificity. It is a commonly used tool in machine learning, signal processing, and other fields that deal with binary classification problems.\n",
    "\n",
    "To create an ROC curve, the classifier system is applied to a dataset with known binary labels (positive and negative cases), and the output scores or probabilities are used to rank the cases in descending order of predicted likelihood of being positive. The ROC curve is then constructed by plotting the true positive rate (TPR) on the y-axis against the false positive rate (FPR) on the x-axis for various threshold values. The TPR is the fraction of positive cases that are correctly identified as positive, while the FPR is the fraction of negative cases that are incorrectly identified as positive.\n",
    "\n",
    "A perfect classifier system would have an ROC curve that passes through the upper left corner (TPR=1, FPR=0) of the plot, indicating perfect sensitivity and specificity. A random classifier system would have an ROC curve that is a diagonal line from (0,0) to (1,1), indicating that the TPR and FPR are equal for all thresholds. An ROC curve that lies above the diagonal line indicates better-than-random classification performance, while a curve that lies below the diagonal line indicates worse-than-random performance.\n",
    "\n",
    "The area under the ROC curve (AUC) is a commonly used metric for evaluating the overall performance of a binary classifier system, with higher values indicating better performance. The ROC curve and AUC can be used to compare different classifier systems, to optimize threshold values, and to visualize the trade-offs between sensitivity and specificity for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1493fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment_08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
