{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 20 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f93234",
   "metadata": {},
   "source": [
    "Submitted By: ANSARI PARVEJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f2249",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) is a supervised machine learning algorithm that is primarily used for classification and regression analysis. The fundamental concept of SVM is to identify a hyperplane or a decision boundary that separates data points belonging to different classes in the feature space. SVM aims to find the optimal hyperplane that maximizes the margin between the decision boundary and the closest points of different classes. The points that lie closest to the hyperplane and influence its position are called support vectors.\n",
    "\n",
    "In the case of nonlinearly separable data, SVM transforms the original input space into a high-dimensional feature space using a kernel function. The data points are then mapped to this feature space, where a linear hyperplane can separate them. By doing so, the SVM algorithm is able to model complex decision boundaries that cannot be achieved with linear models. SVM has been widely used in various applications such as image classification, text classification, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091daf97",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVM), the data points that are closest to the hyperplane or decision boundary and influence its position are known as support vectors. Support vectors are the critical elements in SVM, as they define the optimal hyperplane that separates the data points belonging to different classes with maximum margin.\n",
    "\n",
    "The distance between the support vectors and the decision boundary is called the margin, and the objective of SVM is to maximize this margin. SVM aims to find the optimal hyperplane that separates the data points while maximizing the margin between the support vectors and the decision boundary.\n",
    "\n",
    "In SVM, only the support vectors need to be stored after the training phase, which makes it a memory-efficient algorithm compared to other classification algorithms. The support vectors play a crucial role in the SVM algorithm as they provide a compact representation of the data points and determine the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820cfdce",
   "metadata": {},
   "source": [
    "When using SVMs, it is necessary to scale the inputs because the SVM algorithm tries to maximize the margin between the support vectors and the decision boundary. The scale of the input features can significantly affect the position of the decision boundary, and features with larger scales can dominate the optimization process.\n",
    "\n",
    "For instance, consider a dataset with two features: feature A ranges from 0 to 1, while feature B ranges from 0 to 1000. If we apply the SVM algorithm to this dataset without scaling the features, the decision boundary will be mainly determined by feature B because it has a much larger range than feature A. This can lead to poor generalization and lower accuracy on the test dataset.\n",
    "\n",
    "By scaling the input features, we ensure that each feature has a similar range of values, and the SVM algorithm can optimize the decision boundary based on all features equally. This helps to avoid the dominance of any particular feature and leads to better classification accuracy and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283520c0",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score or probability estimate for a predicted class label. However, the method for obtaining these scores depends on the specific implementation of the SVM algorithm.\n",
    "\n",
    "In general, SVMs are designed to make binary classification decisions, meaning they predict one of two possible class labels for each input instance. The predicted class label is based on which side of the decision boundary the instance falls on. The distance between the instance and the decision boundary can be used as a measure of confidence or certainty for the prediction.\n",
    "\n",
    "One common method for obtaining confidence scores in SVMs is to use the distance between the test instance and the decision boundary as a measure of confidence. Specifically, the SVM algorithm outputs a signed distance value, known as the margin, which measures the separation between the decision boundary and the nearest data point of each class. The larger the margin value, the more confident the SVM is in its prediction.\n",
    "\n",
    "Probability estimates can also be obtained using techniques such as Platt scaling or isotonic regression. These methods aim to map the SVM's signed distance values to a probability value between 0 and 1, which represents the estimated probability that the instance belongs to a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec0a77",
   "metadata": {},
   "source": [
    "When training a model on a large dataset with millions of instances and hundreds of features, it is generally recommended to use the dual form of the SVM problem instead of the primal form.\n",
    "\n",
    "The reason for this is that the primal form of the SVM problem involves solving an optimization problem with the number of variables equal to the number of training instances, which can be computationally expensive and inefficient for large datasets. On the other hand, the dual form of the SVM problem involves solving an optimization problem with the number of variables equal to the number of support vectors, which is typically much smaller than the number of training instances.\n",
    "\n",
    "Therefore, using the dual form of the SVM problem can be more efficient and effective for large datasets with many features. However, it is important to note that the choice between the primal and dual form may depend on the specific characteristics of the problem and the available computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527ebd4",
   "metadata": {},
   "source": [
    "If an SVM classifier underfits the training set, the RBF kernel's hyperparameters may need to be adjusted. The gamma parameter, which regulates the kernel's width, and the C parameter, which determines the trade-off between maximizing the margin and minimizing the classification error, are the two most critical hyperparameters.\n",
    "\n",
    "To enhance the classifier's ability to fit the training set, it is recommended to lower the value of the gamma parameter. A smaller gamma value will lead to a more generalized and less complex model, which may improve the model's ability to fit new data.\n",
    "\n",
    "Regarding the C parameter, increasing it might also help improve the classifier's fitting on the training data. A larger C value will reduce the margin and allow the model to fit the training data better, but it may lead to overfitting. Therefore, a careful balance should be struck between maximizing the margin and minimizing the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b3cca",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "    H: The H matrix should be a diagonal matrix with the values of C on the diagonal and 0 elsewhere, where C is the penalty parameter for the soft margin. If there are n training instances, H should be an n × n matrix.\n",
    "\n",
    "    f: The f vector should be a column vector of length n with all elements set to -1.\n",
    "\n",
    "    A: The A matrix should be an m × n matrix, where m is the number of inequality constraints in the soft margin problem. Each row of the A matrix corresponds to one inequality constraint, and each column corresponds to one training instance. The values in the A matrix depend on the specific inequality constraint being used. For example, if using the standard soft margin constraints, each row of A would be the transpose of the label vector y multiplied by the corresponding feature vector x.\n",
    "\n",
    "    b: The b vector should be a column vector of length m with all elements set to 0.\n",
    "\n",
    "Once these parameters are set, the QP solver can be used to solve the soft margin SVM problem and find the optimal values of the primal variables (w, b) that define the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f716511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 0.88\n",
      "SVC accuracy: 0.88\n",
      "SGDClassifier accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# generate a linearly separable dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train a LinearSVC model\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "print(\"LinearSVC accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# train an SVC model\n",
    "svc = SVC(kernel=\"linear\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"SVC accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# train an SGDClassifier model\n",
    "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(\"SGDClassifier accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d926bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM classifier with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', gamma=0.01, C=1)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc128984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.3570026426754463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM regressor with an RBF kernel\n",
    "svm_reg = SVR(kernel=\"rbf\", gamma=\"scale\")\n",
    "svm_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = svm_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
