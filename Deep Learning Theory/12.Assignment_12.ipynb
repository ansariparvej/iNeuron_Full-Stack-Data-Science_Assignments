{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 12 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d94f9",
   "metadata": {},
   "source": [
    "Submitted By: ANSARI PARVEJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tHow does unsqueeze help us to solve certain broadcasting problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b963132",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "\n",
    "In PyTorch, unsqueeze is a method that allows you to increase the number of dimensions of a tensor by adding one or more dimensions of size 1 in a specified position. This operation can be useful in broadcasting problems, where you have tensors with different shapes and you want to perform element-wise operations between them.\n",
    "\n",
    "Broadcasting is a technique in PyTorch that allows tensors with different shapes to be used in arithmetic operations. It works by expanding one or both tensors so that they have the same shape. When the shapes differ, PyTorch automatically broadcasts the smaller tensor to match the larger tensor's shape. However, this broadcasting only works if the tensor dimensions match or if one of the tensors has a dimension of size 1.\n",
    "\n",
    "This is where unsqueeze can be useful. By adding a dimension of size 1 to a tensor, you can effectively match its shape with another tensor's shape, allowing for element-wise operations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tHow can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c80d7",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "We can use indexing to achieve the same operation as unsqueeze. To do this, we can create a new axis with a size of 1 using the indexing syntax.\n",
    "\n",
    "For example, let's say we have a tensor x with shape (3, 4). To add a new axis with a size of 1 at the end of the tensor, we can use the following indexing syntax:\n",
    "\n",
    "- x[:, :, None]\n",
    "\n",
    "Here, None creates a new axis with a size of 1. The resulting tensor will have shape (3, 4, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tHow do we show the actual contents of the memory used for a tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119279f",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "To show the actual contents of the memory used for a tensor in PyTorch, we can use the numpy() method of the tensor object to convert it to a NumPy array, and then print the array.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create a tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# convert the tensor to a NumPy array and print it\n",
    "print(x.numpy())\n",
    "\n",
    "# output: [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhen adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "import torch\n",
    "\n",
    "# create a matrix and a vector\n",
    "m = torch.ones(3, 3)\n",
    "v = torch.tensor([1, 2, 3])\n",
    "\n",
    "# add the vector to the matrix\n",
    "result = m + v\n",
    "\n",
    "# print the result\n",
    "print(result)\n",
    "\n",
    "# output: \n",
    "\n",
    "tensor([[2., 2., 2.],\n",
    "        [2., 2., 2.],\n",
    "        [2., 2., 2.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tDo broadcasting and expand_as result in increased memory use? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9201e40",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Broadcasting and expand_as do not result in increased memory use for the output tensor, as they do not create new memory allocations for the result. Instead, they manipulate the strides and shape of the output tensor to allow for element-wise operations with other tensors without copying or reallocating the data in memory.\n",
    "\n",
    "Broadcasting modifies the shape and strides of the input tensors to enable them to be combined element-wise. This is done without actually creating a new tensor or allocating any additional memory. Instead, the new tensor shape and strides are computed dynamically at runtime, allowing the tensor to be used in element-wise operations with other tensors without creating a copy.\n",
    "\n",
    "Similarly, the expand_as method creates a new tensor with the same data as the original tensor, but with additional dimensions added according to the shape of the provided tensor argument. Again, this is done without allocating any additional memory for the result tensor. Instead, the new tensor is created with the same underlying storage as the original tensor, but with different shape and strides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tImplement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6dc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "import torch\n",
    "\n",
    "# create two matrices\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "# matrix multiplication using Einstein summation\n",
    "C = torch.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "# print the result\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat does a repeated index letter represent on the lefthand side of einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8eb4f7",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "When a repeated index letter appears on the left-hand side of an Einstein summation notation expression, it indicates that a sum should be taken over that index.\n",
    "\n",
    "For example, consider the expression ii->, which computes the sum of the diagonal elements of a square matrix. The repeated index letter i on the left-hand side indicates that a sum should be taken over the diagonal elements of the input matrix. The notation -> on the right-hand side indicates that the output should be a scalar, i.e., a tensor with no dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhat are the three rules of Einstein summation notation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db424ba",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "- Repeated indices are summed over.\n",
    "- Free indices on the left-hand side of the expression correspond to output indices on the right-hand side.\n",
    "- Each index can appear at most twice in a given expression.\n",
    "\n",
    "These rules are used to define tensor contraction operations, which are fundamental operations in tensor algebra. The Einstein summation notation provides a concise and efficient way to express these operations using index notation.\n",
    "\n",
    "The first rule states that repeated indices are summed over. This means that if an index appears twice in an expression, then the expression represents a sum over that index. For example, the expression A_i B_i represents the sum of the element-wise products of the vectors A and B.\n",
    "\n",
    "The second rule states that free indices on the left-hand side of the expression correspond to output indices on the right-hand side. This means that if an index appears on the left-hand side of the expression but not on the right-hand side, then it is an index of the output tensor. For example, the expression A_i B_j represents a matrix multiplication, with the output tensor having indices (i, j).\n",
    "\n",
    "The third rule states that each index can appear at most twice in a given expression. This means that the expression cannot have more than two occurrences of any index, since each occurrence represents a summation over that index. For example, the expression A_i B_i C_j is not valid, since the index i appears three times.\n",
    "\n",
    "These rules provide a consistent and concise notation for manipulating tensors in linear algebra, and are used extensively in various scientific fields and in machine learning frameworks like PyTorch and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tWhat are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db2fdf",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "The forward pass and backward pass are two key steps in the training of a neural network.\n",
    "\n",
    "The forward pass is the computation of the output of the neural network for a given input. The input is fed into the network, and the network applies a series of mathematical operations (such as linear transformations, non-linear activations, pooling, etc.) to the input data to compute the output. The forward pass is essentially the process of propagating the input through the neural network to compute the output.\n",
    "\n",
    "The backward pass, also known as backpropagation, is the process of computing the gradients of the loss function with respect to the weights of the neural network. In other words, it is the process of computing how much each weight contributed to the error in the output, and adjusting the weights accordingly to reduce the error. The gradients are computed using the chain rule of calculus, which involves computing the gradients of the output with respect to each intermediate variable in the network. The backward pass is essentially the process of propagating the error backwards through the network to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c90c5",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Storing some of the activations calculated for intermediate layers in the forward pass is necessary for performing the backward pass, also known as backpropagation, which is a crucial step in training neural networks.\n",
    "\n",
    "In the backward pass, we need to compute the gradients of the loss function with respect to the weights of the neural network. This involves computing the gradients of the output with respect to each intermediate variable in the network. These intermediate variables correspond to the activations of the neurons in the hidden layers of the network.\n",
    "\n",
    "To compute the gradients of the output with respect to these intermediate variables, we need to store the intermediate activations during the forward pass. Specifically, we need to store the activations for each layer that will be used in the backward pass for computing the gradients. These activations are then used in the backward pass to compute the gradients of the loss function with respect to the weights.\n",
    "\n",
    "If we did not store these intermediate activations during the forward pass, we would not be able to compute the gradients for the hidden layers, and therefore would not be able to train the neural network. Storing these intermediate activations allows us to efficiently compute the gradients during backpropagation and update the weights of the network to minimize the loss function.\n",
    "\n",
    "Therefore, storing some of the activations calculated for intermediate layers in the forward pass is crucial for training neural networks and is a necessary step in the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860f8b5",
   "metadata": {},
   "source": [
    "#### 11.\tWhat is the downside of having activations with a standard deviation too far away from 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6c8d6",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Having activations with a standard deviation that is too far away from 1 can lead to a number of issues in training neural networks.\n",
    "\n",
    "When the standard deviation of the activations is too small (i.e., close to 0), the activations become too flat and the gradients can vanish during backpropagation. This means that the gradients become so small that they cannot effectively update the weights of the network, leading to slow convergence or even stagnation of the training process. This is known as the vanishing gradient problem.\n",
    "\n",
    "On the other hand, when the standard deviation of the activations is too large, the activations become too spread out and the gradients can explode during backpropagation. This means that the gradients become so large that they cause the weights to update too much, leading to unstable training and divergence of the training process. This is known as the exploding gradient problem.\n",
    "\n",
    "In both cases, having activations with a standard deviation too far away from 1 can lead to unstable training and poor performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713869",
   "metadata": {},
   "source": [
    "#### 12.\tHow can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb4787",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Weight initialization is an important technique for avoiding the problem of vanishing or exploding gradients in neural networks. By initializing the weights of the network appropriately, we can ensure that the activations have a reasonable standard deviation and avoid the problems associated with having activations that are too small or too large.\n",
    "\n",
    "One common approach to weight initialization is to use a Gaussian distribution with mean 0 and standard deviation that depends on the size of the input and output layers of the weight matrix. For example, the Xavier initialization method initializes the weights with a Gaussian distribution with mean 0 and standard deviation sqrt(2 / (n_in + n_out)), where n_in and n_out are the number of input and output neurons in the weight matrix, respectively. This ensures that the variance of the activations is approximately 1, which helps to avoid the problems of vanishing or exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8b7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
