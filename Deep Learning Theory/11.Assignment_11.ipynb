{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 11 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b73ce",
   "metadata": {},
   "source": [
    "Submitted By: ANSARI PARVEJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWrite the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65817af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Ans:**\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        output = self.activation(z)\n",
    "        return output\n",
    "        \n",
    "    def activation(self, z):\n",
    "        # sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWrite the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195c4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tWrite the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e3f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83a0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[0.0 for j in range(output_size)] for i in range(input_size)]\n",
    "        self.bias = [0.0 for i in range(output_size)]\n",
    "        for i in range(input_size):\n",
    "            for j in range(output_size):\n",
    "                self.weights[i][j] = random.uniform(-1, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = [sum([inputs[i] * self.weights[i][j] for i in range(len(inputs))]) + self.bias[j] for j in range(len(self.bias))]\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat is the “hidden size” of a layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12707c",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "The \"hidden size\" of a layer in a neural network refers to the number of neurons in that layer. It is called \"hidden\" because these neurons are not directly visible as input or output to the network's input or output layers. Instead, they are used to transform the input into a form that is more useful for the network to learn from, or to transform the output from the previous layer into a form that is suitable as input for the next layer.\n",
    "\n",
    "The hidden size of a layer is a hyperparameter that must be specified prior to training the network. A larger hidden size may allow the network to model more complex relationships in the data, but may also lead to overfitting or slow training. A smaller hidden size may lead to underfitting, as the network may not be able to model the data as well. Determining the optimal hidden size is often a matter of trial and error, and can depend on factors such as the complexity of the task, the size of the input data, and the available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tWhat does the t method do in PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c81bb6",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "In PyTorch, the t() method is used to transpose a tensor. It returns a tensor with the dimensions permuted such that the original tensor's columns become its rows, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhy is matrix multiplication written in plain Python very slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ea7cb",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Matrix multiplication is a computationally intensive operation that requires a large number of operations to be performed on each element of the output matrix. When implemented in plain Python, these operations are performed in a serial manner, one at a time, using nested loops or list comprehensions. This approach can be very slow for large matrices because Python's interpreted nature and dynamic typing result in a significant overhead for each operation.\n",
    "\n",
    "On the other hand, most numerical computing libraries, such as NumPy or PyTorch, are implemented in lower-level languages like C or C++, which can execute operations much faster than Python. These libraries are also optimized for matrix operations, with specialized algorithms that take advantage of the underlying hardware and minimize memory access, making them much faster than plain Python implementations.\n",
    "\n",
    "In addition, these libraries often leverage hardware acceleration such as GPUs or specialized tensor processing units (TPUs) to further speed up matrix multiplication and other numerical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tIn matmul, why is ac==br?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c06d4f",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "In matmul, ac==br is a requirement for matrix multiplication because it ensures that the matrices are conformable for multiplication.\n",
    "\n",
    "Matrix multiplication requires that the number of columns in the first matrix matches the number of rows in the second matrix. In the expression C = A @ B, if A has shape (a, b) and B has shape (c, d), then the number of columns in A (i.e., b) must be equal to the number of rows in B (i.e., c), so that the resulting matrix C has shape (a, d).\n",
    "\n",
    "In the expression C = torch.matmul(A, B), torch.matmul requires that the input tensors have at least 2 dimensions and that the number of columns of A is equal to the number of rows of B. This means that the shapes of the input tensors must satisfy the condition ac==br for matrix multiplication to be possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680d632",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "In Jupyter Notebook, you can use the %timeit magic command to measure the time taken for a single cell to execute. Simply start the cell with %timeit and then write the code you want to measure. The %timeit command will execute the code multiple times and provide the average execution time. For example:\n",
    "\n",
    "- %timeit my_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWhat is elementwise arithmetic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd335fa4",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Elementwise arithmetic refers to the operation of performing arithmetic operations (such as addition, subtraction, multiplication, division, etc.) on corresponding elements of two or more arrays or tensors, one element at a time. This means that each element of the output array or tensor is calculated by applying the arithmetic operation to the corresponding elements of the input arrays or tensors.\n",
    "\n",
    "Elementwise arithmetic operations are commonly used in deep learning for tasks such as normalization, activation functions, and loss calculations. In many cases, these operations can be implemented efficiently using hardware-accelerated routines such as those provided by GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2fdc0",
   "metadata": {},
   "source": [
    "#### 11.\tWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dff1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Ans:**\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "result = torch.all(a > b)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ba3dd",
   "metadata": {},
   "source": [
    "#### 12.\tWhat is a rank-0 tensor? How do you convert it to a plain Python data type?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31ae2b",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "In TensorFlow and PyTorch, a rank-0 tensor is also known as a scalar tensor or a 0-dimensional tensor. It is a tensor that has zero dimensions and contains a single value.\n",
    "\n",
    "To convert a rank-0 tensor to a plain Python data type, you can use the item() method in PyTorch or the numpy() method in TensorFlow. Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a rank-0 tensor\n",
    "a = torch.tensor(42)\n",
    "\n",
    "# Convert the rank-0 tensor to a plain Python int\n",
    "b = a.item()\n",
    "\n",
    "print(type(b)) # Output: <class 'int'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e79117",
   "metadata": {},
   "source": [
    "#### 13.\tHow does elementwise arithmetic help us speed up matmul?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8a7b1",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Elementwise arithmetic is a useful optimization for speeding up matrix multiplication because it allows us to perform certain operations more efficiently. Specifically, we can use elementwise multiplication to perform broadcasting, which allows us to multiply a matrix by a vector or a scalar without actually creating the full matrix explicitly.\n",
    "\n",
    "For example, suppose we want to multiply a 3x3 matrix A by a scalar x. Instead of creating a full 3x3 matrix of x values, we can simply multiply each element of A by x using elementwise multiplication. This reduces the amount of memory needed and speeds up the computation.\n",
    "\n",
    "Another example is multiplying a matrix A by a vector v, where v has the same number of columns as A. In this case, we can use broadcasting to perform the multiplication efficiently. Specifically, we can multiply each row of A by v using elementwise multiplication and then sum the results to get the final output. This is much faster than explicitly creating a full matrix and then performing matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43354d",
   "metadata": {},
   "source": [
    "#### 14.\tWhat are the broadcasting rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68682fe4",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Broadcasting rules refer to how NumPy handles elementwise operations between arrays with different shapes. The broadcasting rules are as follows:\n",
    "\n",
    "- If the arrays have different numbers of dimensions, the smaller one is padded with ones on its left side until the numbers of dimensions match.\n",
    "\n",
    "- If the sizes of the two arrays along a particular dimension are different and one of them is 1, then the array with size 1 is stretched to match the size of the other array along that dimension.\n",
    "\n",
    "- If the sizes of the two arrays are different and neither is 1, then the operation is not possible and an error is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5204b7",
   "metadata": {},
   "source": [
    "#### 15.\tWhat is expand_as? Show an example of how it can be used to match the results of broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d3c48",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "**expand_as** is a PyTorch method that allows you to expand a tensor to match the size of another tensor. This is useful when you need to perform elementwise operations between two tensors of different sizes, but they are broadcastable.\n",
    "\n",
    "Here is an example of how expand_as can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define two tensors of different sizes\n",
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "b = torch.tensor([10, 20])\n",
    "\n",
    "# Add the tensors together\n",
    "c = a + b.expand_as(a)\n",
    "\n",
    "# Print the tensors\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
