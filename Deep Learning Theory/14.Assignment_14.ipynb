{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 14 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161bf3c",
   "metadata": {},
   "source": [
    "Submitted By: ANSARI PARVEJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cb1d1",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "No, it is not okay to initialize all the weights to the same value, even if that value is randomly selected using He initialization. The reason is that each neuron should learn to detect different features from the input data, and if all the weights are the same, all the neurons will be computing the same function, making the neural network inefficient. Initializing the weights randomly ensures that each neuron will learn to detect different features. Additionally, He initialization only determines the scale of the initial weights and does not ensure that they are diverse or unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56827be1",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Yes, it is generally okay to initialize the bias terms to 0. Initializing the bias terms to 0 does not cause any issues as long as the weights are initialized with appropriate values. It is also a common practice to initialize the biases to 0 in order to reduce the number of hyperparameters that need to be tuned during training. However, there are cases where initializing the biases to non-zero values may be beneficial, such as in the case of ReLU activation functions, where a small positive bias can help avoid \"dead\" neurons that are stuck in the zero region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b97e32",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Here are three advantages of the Exponential Linear Unit (ELU) activation function over the Rectified Linear Unit (ReLU) activation function:\n",
    "\n",
    "- Handles negative inputs better: The ELU activation function is capable of handling negative inputs more effectively than the ReLU function, since it has a nonzero gradient for negative inputs. This can help prevent the \"dying ReLU\" problem where certain neurons become inactive and stop learning.\n",
    "\n",
    "- Smoother derivative: The ELU activation function has a smoother derivative than the ReLU function, making it more well-behaved and easier to optimize with certain gradient descent methods.\n",
    "\n",
    "- Produces zero mean activations: The ELU activation function produces activations with zero mean, which can help speed up learning and prevent overfitting by reducing the covariate shift during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934f6de",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "- ELU: The Exponential Linear Unit (ELU) activation function is a good choice when you want to avoid the \"dying ReLU\" problem and handle negative inputs more effectively. It is often recommended as a default choice.\n",
    "\n",
    "- Leaky ReLU and its variants: Leaky ReLU activation function can be used in cases where ReLU leads to sparse activation or dying ReLU problem. Leaky ReLU has a small positive slope for negative inputs and is relatively easy to implement. Parametric ReLU (PReLU) and its variants take this idea further by making the slope a learnable parameter.\n",
    "\n",
    "- ReLU: Rectified Linear Unit (ReLU) activation function is a popular choice for most cases, as it is simple and computationally efficient. It is a good starting point for most neural network architectures, but be mindful of the \"dying ReLU\" problem when using it.\n",
    "\n",
    "- tanh: The hyperbolic tangent (tanh) activation function is often used in recurrent neural networks (RNNs) and sequence-to-sequence models, since it is capable of modeling complex temporal dependencies. It is also a good choice when you need activations in the range [-1, 1].\n",
    "\n",
    "- logistic: The logistic activation function is commonly used in binary classification problems, where you want to produce probabilities that the input belongs to a particular class. It is also used in autoencoders and other generative models.\n",
    "\n",
    "- softmax: The softmax activation function is used in the output layer of a neural network when you want to perform multi-class classification. It takes a vector of arbitrary real-valued scores and produces a vector of probabilities that add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba98d3",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "If the momentum hyperparameter is set too close to 1 (e.g., 0.99999) when using a MomentumOptimizer, it may lead to the optimizer overshooting the minimum of the cost function and bouncing back and forth, making the optimization process unstable. This is because the momentum term has a very strong influence on the optimization process, causing the optimizer to rely heavily on the previous gradient directions and momentum values, rather than the current gradient. This can cause the optimizer to take longer to converge, or in some cases, prevent it from converging at all. It is generally recommended to set the momentum hyperparameter to a value between 0.9 and 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2aa64",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Here are three ways to produce a sparse model:\n",
    "\n",
    "- L1 regularization: Adding an L1 penalty term to the loss function during training encourages the model to learn sparse weights, as it drives many weights to exactly zero. This technique is also known as Lasso regression.\n",
    "\n",
    "- Dropout: During training, randomly setting a fraction of the neurons in a layer to zero forces the remaining neurons to be more robust and less co-dependent. This technique can produce a sparse model by setting some weights to zero during training.\n",
    "\n",
    "- Weight pruning: This technique involves training a model to convergence, and then removing the smallest weights (in absolute value) from the network. By iteratively pruning the smallest weights, a sparse model can be produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd6576",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "Yes, dropout can slow down training since it randomly drops out neurons, which means that the remaining neurons need to compensate for the missing ones, leading to longer training times. However, dropout does not slow down inference since all neurons are used during inference, and their weights are scaled accordingly to compensate for the dropout during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
